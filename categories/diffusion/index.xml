<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Diffusion on Xiong Jia</title>
        <link>https://Peraspera1.github.io/categories/diffusion/</link>
        <description>Recent content in Diffusion on Xiong Jia</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>xj</copyright>
        <lastBuildDate>Tue, 04 Mar 2025 16:22:05 +0800</lastBuildDate><atom:link href="https://Peraspera1.github.io/categories/diffusion/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Cosmos</title>
        <link>https://Peraspera1.github.io/p/cosmos/</link>
        <pubDate>Tue, 04 Mar 2025 16:22:05 +0800</pubDate>
        
        <guid>https://Peraspera1.github.io/p/cosmos/</guid>
        <description>&lt;img src="https://Peraspera1.github.io/images/cosmos/cover.png" alt="Featured image of post Cosmos" /&gt;&lt;h2 id=&#34;cosmoscomfyui-环境配置记录&#34;&gt;Cosmos+ComfyUI 环境配置记录
&lt;/h2&gt;&lt;h3 id=&#34;配置comfyui-环境&#34;&gt;配置ComfyUI 环境
&lt;/h3&gt;&lt;p&gt;官方推荐的是python=3.12版本，但是由于我的cuda是11.7版本，没有对应的pytorch，所以我改成了3.11版本的python及其对应的pytorch，这里的版本可根据需要自行修改。&lt;/p&gt;
&lt;p&gt;在终端中输入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;conda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;create&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;comfyui&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;python&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.11&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;conda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;activate&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;comfyui&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;conda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pytorch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.5.1&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torchvision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.15.2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torchaudio&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.0.2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pytorch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;12.1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pytorch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nvidia&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;git&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clone&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;github&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;com&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;comfyanonymous&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ComfyUI&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;git&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ComfyUI&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;r&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requirements&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;txt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pypi&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mirrors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ustc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;edu&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simple&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;之后运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;python&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;py&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;出现如下结果：
&lt;img src=&#34;https://Peraspera1.github.io/images/cosmos/comfyui1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;终端截图&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;打开网站后结果如下
&lt;img src=&#34;https://Peraspera1.github.io/images/cosmos/comfyui2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;网页UI&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;至此comfyUI配置完成&lt;/p&gt;
&lt;h3 id=&#34;cosmos-相关配置&#34;&gt;Cosmos 相关配置
&lt;/h3&gt;&lt;p&gt;下载 &lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/tree/main/text_encoders&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;语言模型&lt;/a&gt;
并保存到ComfyUI/models/text_encoders/ 路径下&lt;/p&gt;
&lt;p&gt;下载 &lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/blob/main/vae/cosmos_cv8x8x8_1.0.safetensors&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;vae&lt;/a&gt;
并保存到 ComfyUI/models/vae/ 路径下&lt;/p&gt;
&lt;p&gt;下载 &lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/mcmonkey/cosmos-1.0/blob/main/Cosmos-1_0-Diffusion-7B-Text2World.safetensors&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;cosmos模型&lt;/a&gt;
并保存到 ComfyUI/models/diffusion_models 路径下&lt;/p&gt;
&lt;h3 id=&#34;comfyui-工作流配置&#34;&gt;ComfyUI 工作流配置
&lt;/h3&gt;&lt;p&gt;在&lt;a class=&#34;link&#34; href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/cosmos/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官方文档&lt;/a&gt;中，下载对应的json文件并保存到comfyui下的workflow的文件夹（自己新建），下图以text2video为例&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Peraspera1.github.io/images/cosmos/comfyui3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;workflow&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;然后在网页端打开这个工作流直接运行就可以了&lt;/p&gt;
&lt;h3 id=&#34;踩坑&#34;&gt;踩坑
&lt;/h3&gt;&lt;p&gt;Q1&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;module&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;torch&amp;#39;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;has&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attribute&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;float8_e4m3fn&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;A1&lt;/p&gt;
&lt;p&gt;torch版本太低了，升级到pytorch=2.5.1+cuda=12.1就好&lt;/p&gt;
&lt;h1 id=&#34;comfyui-基本使用方法&#34;&gt;Comfyui 基本使用方法
&lt;/h1&gt;&lt;p&gt;安装了两个插件还挺好用的，分别是AlekPet和VHS&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Phys_Summary</title>
        <link>https://Peraspera1.github.io/p/phys_summary/</link>
        <pubDate>Thu, 27 Feb 2025 12:49:57 +0800</pubDate>
        
        <guid>https://Peraspera1.github.io/p/phys_summary/</guid>
        <description>&lt;img src="https://Peraspera1.github.io/images/summary_cover.png" alt="Featured image of post Phys_Summary" /&gt;&lt;p&gt;尽管当前的视频生成模型已能产生令人满意的效果，但生成的结果中仍常出现不自然的现象(&lt;a class=&#34;link&#34; href=&#34;https://openai.com/index/sora/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Sora&lt;/a&gt;)，这些现象往往违背了我们对几何和物理常识的理解，因此近年来，许多研究尝试将物理规律融入视频生成模型来得到更好的结果。与此同时，在机器人领域，大多数研究仍主要聚焦于机器人自身的运动轨迹生成，而对环境及交互物体的感知仍停留在视觉信息的层面，未能充分利用物理规律。因此，这篇文章总结了我对近期工作的一些总结，以及一点点对于这些工作在灵巧手操作上可能应用的思考。&lt;/p&gt;
&lt;p&gt;我的想法大致如下：
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/overview.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;overview&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;整个工作的pipeline如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; 给定一个任务，利用LLM预测我们需要知道哪些物理属性（比如物体的摩擦系数）；&lt;/li&gt;
&lt;li&gt; 输入一段视频作为监督信号，从该视频的初始帧开始进行物理仿真，优化物理参数场；&lt;/li&gt;
&lt;li&gt; 得到较为精确的物理参数后，生成/仿真得到该物体在被施加一个力后的响应；&lt;/li&gt;
&lt;li&gt; 在已有的手物交互轨迹基础上，如果能准确预测物体对外力的响应，就能进一步微调手的施力方向和大小，或者将这些信息直接融入轨迹生成过程中，以提升交互精度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一句话概括就是让机器人理解真实环境中的物理属性，从而更好地辅助操作实现。那么自然而然地会引出两个问题，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先，如何从环境中恢复物体的物理属性并进行物理仿真？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其次，在机器人已知物体的物理属性后，如何利用这些信息优化灵巧手的操作？&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;physics-simulation--video-generation&#34;&gt;Physics Simulation &amp;amp; Video Generation
&lt;/h2&gt;&lt;p&gt;事实上，&lt;strong&gt;给定视频恢复物理属性&lt;/strong&gt;(pipeline2)与&lt;strong&gt;给定物理属性生成视频&lt;/strong&gt;(pipeline3)互为反问题(不过我更关注的是生成物体对于外力的响应，并不需要那种精细的视频)。然而，近年来学界的研究更倾向于后者，这一领域的研究主要分为以下三个方向：&lt;/p&gt;
&lt;h3 id=&#34;物理对齐-physics-alignment&#34;&gt;物理对齐 Physics Alignment
&lt;/h3&gt;&lt;p&gt;在语言模型中，对齐（Alignment）指的是通过一系列算法和工程手段，修正模型的行为，使其输出符合预设的安全边界和人类意图。与之对应的，修正视频模型的输出使其满足物理规则的过程就是物理对齐。作为对齐领域的代表性工作，&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;InstructGPT&lt;/a&gt; 提出了两种对齐方法：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;监督微调&lt;/strong&gt;（Supervised Fine Tuning，&lt;strong&gt;SFT&lt;/strong&gt;）人工标注高质量的提示（prompt）和回答（output）数据集，通过监督学习的方式微调模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基于人类反馈的强化学习&lt;/strong&gt;（Reinforcement Learning from Human Feedback，&lt;strong&gt;RLHF&lt;/strong&gt;）对同一个提示，模型输出多个回答，人工对这些回答进行比较打分；使用打分的结果训练一个反馈模型（Reward Model），用于评价模型输出的好坏；使用反馈模型对模型进行强化学习，比如近端策略优化（Proximal Policy Gradient，PPO）。&lt;/p&gt;
&lt;p&gt;将这两种想法用于视频模型中是比较直观的。对于 SFT，我们就需要使用物理真实的视频作为输入。但是真实世界中的视频当然都是物理真实的，可能的问题是视频动态不够，导致模型没有接受到足够的动态信息。因此像 &lt;a class=&#34;link&#34; href=&#34;https://github.com/NVIDIA/Cosmos&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cosmos&lt;/a&gt; 在预训练阶段就会保证数据能够反映真实物理规则。这主要是通过两点做到的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; 收集包含大量动态的视频：包括驾驶视频、手部动作、第一人称视角、模拟结果等等。&lt;/li&gt;
&lt;li&gt; 对数据进行过滤：剔除质量低的、缺乏动态的、非物理等的视频，并提取一部分高质量视频作为后训练数据集。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;尽管 Cosmos 的&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2501.03575&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;论文&lt;/a&gt;在 5.3.2 节专门讨论了物理对齐的问题，但是实际上并没有做更多的尝试，只是在几个场景中测试了 Cosmos 生成的结果是否吻合模拟/真实的物理。&lt;/p&gt;
&lt;p&gt;对于 RLHF 而言，首要问题是需要一个反馈模型来判别模型输出的结果是否满足物理规律。这方向一个代表工作是 &lt;a class=&#34;link&#34; href=&#34;https://videophy.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;VideoPhy&lt;/a&gt;。这篇工作的核心是对市面上十二个视频模型的生成结果进行人工打分，然后训练一个打分网络 VideoCon-Physics。打分分为两个维度，每个维度得分只有 0 或 1：一个是语义的符合程度（Semantic Adherence，SA），一个是是否符合物理常识（Physical Commonsense，PC），结果如下：
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/video_phy.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Benchmark&#34;
	
	
&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:14px;color:#C0C0C0;text-decoration:underline&#34;&gt;Benchmark&lt;/center&gt; 
&lt;p&gt;可以发现成绩最好的是开源模型 &lt;a class=&#34;link&#34; href=&#34;https://github.com/THUDM/CogVideo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CogVideoX-5B&lt;/a&gt;，但是也只是勉强及格的水平。这方向类似的工作还有 &lt;a class=&#34;link&#34; href=&#34;https://tiger-ai-lab.github.io/VideoScore/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;VideoScore&lt;/a&gt;，&lt;a class=&#34;link&#34; href=&#34;https://phygenbench2024.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhyGenBench&lt;/a&gt;。理论上有了打分模型之后我们就可以对视频模型进行强化学习对齐了，&lt;a class=&#34;link&#34; href=&#34;https://onlinevpo.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OnlineVPO&lt;/a&gt; 就使用了 VideoScore 作为反馈模型微调了 &lt;a class=&#34;link&#34; href=&#34;https://github.com/hpcaitech/Open-Sora&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenSora&lt;/a&gt; 模型，使其在 VideoScore 得分上超越了其他模型。&lt;/p&gt;
&lt;p&gt;整体上来说，物理对齐比较依赖预训练大模型的能力。对于语言模型来说，对齐往往会降低模型在基准测试上的分数，称为支付对齐税（Alignment Tax）。对于视频模型情况应该是类似的，增强其在物理动态方面的能力可能导致其他能力的削弱。因此，一个更本质的问题是，通过预训练的方式大模型是否能够足够泛化地学到物理规律？字节的工作 &lt;a class=&#34;link&#34; href=&#34;https://phyworld.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How Far is Video Generation from World Model? A Physical Law Perspective&lt;/a&gt; 是这个方向的一个初步探索。&lt;/p&gt;
&lt;h3 id=&#34;二维平面模拟-2d-physics&#34;&gt;二维平面模拟 2D Physics
&lt;/h3&gt;&lt;p&gt;Ok，如果视频模型短期内无法达到我们对于物理规律的需求，那我们是否可以通过加入物理模拟的方式增强这方面的能力呢？由于视频模型都是 2D 的，我们可以先从二维平面上的模拟开始，这一领域分为两个方向，一个是显式地加入物理规律并在屏幕空间上进行模拟，另一个则是隐式地推断物体的动力学信息。&lt;/p&gt;
&lt;p&gt;屏幕空间模拟（Screen-space Simulation）指的是绕过 3D 模型，直接在屏幕空间中模拟物体的动态。这方面的一个代表性工作是 &lt;a class=&#34;link&#34; href=&#34;https://stevenlsw.github.io/physgen/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysGen&lt;/a&gt;。
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/physgen.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Phygen&#34;
	
	
&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:14px;color:#C0C0C0;text-decoration:underline&#34;&gt;Physgen 流程图&lt;/center&gt; 
&lt;p&gt;大致的流程分为三步：1. 给定一张初始图片，我们先做分割，并使用图像理解模型获取法向贴图、语义等信息，然后使用大语言模型推测对应的材质；2. 使用屏幕空间的 2D 模拟器进行模拟（刚体模拟）；3. 使用视频模型将模拟结果与法向贴图等整合起来，得到最终视频。可以发现，这个流程中其实并不需要大模型生成动态，大模型提供的是一个满足时间连续性的&amp;quot;渲染器&amp;quot;，将模拟结果渲染成视频。最近的 &lt;a class=&#34;link&#34; href=&#34;https://xpandora.github.io/PhysAnimator/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysAnimator&lt;/a&gt; 也是这个思路，不过将模拟的对象进一步扩展到了布料这样的软性材料。&lt;/p&gt;
&lt;p&gt;当然，也有不显式加入物理模拟的方式来实现图片上的动力学的工作，而是通过分析图片中物体的振动规律得到信息，并利用这些信息来推断物体的物理特性和驱动它们运动的力，这一工作最早可以追溯到Davis的博士论文&lt;a class=&#34;link&#34; href=&#34;https://dspace.mit.edu/handle/1721.1/107330&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Visual vibration analysis&lt;/a&gt;。如果有了物体对于一个已知力的响应，那么我们是不是就可以在不知道物理规律的前提下分析一个物体对于未知的力的响应了呢（类似于反馈-控制，黑盒模型）？2024年的CVPR Best Paper &lt;a class=&#34;link&#34; href=&#34;https://generative-dynamics.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Generative Image Dynamics&lt;/a&gt; 就做了这样的一件事。与PhysGen不同，这篇论文并没有显式地建模出物体的运动规律，而是只学习到了振动频率(事实上，物体振动频率满足该规律：$\omega = \sqrt{\frac{k}{m}}$)，即学习每个像素点的振动规律(利用傅里叶变换分解，学习的参数是傅里叶的系数)。在不考虑训练成本的情况下，相比于显式地模拟，这种方式的推理速度肯定更快，但是显然只能局限于振动这种简单的物理规律，当然，通过改变基函数的方式（比如把傅里叶级数换成勒让德级数）也许可以将该方法推广到更多真实情景，这也是一个值得探索的方向。&lt;/p&gt;
&lt;p&gt;总的来说，在二维平面上的模拟的优点很明显，我们能够对生成的结果进行非常精确的控制，并且在小幅度内基本满足我们对物理规则的认知。但是缺点同样很明显，由于我们是在屏幕空间做分割和模拟，我们永远只能生成物体一面的结果，像布料的遮挡褶皱也无法处理。并且，生成结果的视角只能是固定的。这使得这类方法只适用于生成动态壁纸这样比较受限的应用，不能作为通用的视频生成方法。&lt;/p&gt;
&lt;p&gt;不过我觉得&lt;a class=&#34;link&#34; href=&#34;https://stevenlsw.github.io/physgen/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysGen&lt;/a&gt;的思维范式很好，如果要继续往下做的话我更倾向于在该工作上的框架上去做。&lt;/p&gt;
&lt;h3 id=&#34;三维空间模拟-3d-physics&#34;&gt;三维空间模拟 3D Physics
&lt;/h3&gt;&lt;p&gt;既然 2D 的模拟终究是妥协，那不如我们直接回到三维模拟。回顾传统的图形管线，生成视频的过程大致可以分为准备3D 资产、进行模拟、渲染结果这三步。这三步中预训练的视频模型可以充当一个非常好的渲染器，比如下图展示的，&lt;a class=&#34;link&#34; href=&#34;https://github.com/NVIDIA/Cosmos&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cosmos&lt;/a&gt; 可以将三维模拟的结果风格迁移到真实场景的&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=9Uch931cDx8&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;视频&lt;/a&gt;。
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/cosmos1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Cosmos1&#34;
	
	
&gt;
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/cosmos2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Cosmos2&#34;
	
	
&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:14px;color:#C0C0C0;text-decoration:underline&#34;&gt; Cosmos中的风格迁移&lt;/center&gt; 
&lt;p&gt;在有 3D 资产和物理参数的基础上，模拟也不是一个困难的问题。传统模拟算法在平衡模拟效果和速度上已经提供了非常多的选择。因此最大的问题在于第一步：对于用户给定的一个语言提示，或者是初始帧，如何获取对应的 3D 资产。对应这两种情况我们可以看到两种解决方法，一是训练文本生成 3D 资产的模型，二是从真实图片中重建。&lt;/p&gt;
&lt;p&gt;首先，世面上已经有很多专注做文本生成 3D 资产的 AI，比如 &lt;a class=&#34;link&#34; href=&#34;https://hyper3d.ai/?lang=zh&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Rodin&lt;/a&gt;，&lt;a class=&#34;link&#34; href=&#34;https://www.meshy.ai/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Meshy&lt;/a&gt; 等,可以直接将这些模型导入到像 Houdini、Blender 这样的图形软件中进行模拟。之前受到很多关注的 &lt;a class=&#34;link&#34; href=&#34;https://genesis-embodied-ai.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Genesis&lt;/a&gt; 想做的就是这个思路。另一方面，过程建模（Procedural Modeling）使用形式语言或者节点化的方式描述模型的生成过程，可以将 3D 模型与文本直接联系起来。比如 SVG 图片使用 html 标记语言，CAD 模型可以完全用代码表示，Houdini 用节点系统描述模型等等。在有了代码化的描述之后，我们就可以通过语言模型去生成这些代码，也就生成了 3D 模型。&lt;a class=&#34;link&#34; href=&#34;https://infinigen.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Infinigen&lt;/a&gt; 通过 Blender 构建了描述自然和室内场景的过程建模语言，因此可以实现文本生成三维场景。&lt;a class=&#34;link&#34; href=&#34;https://gpt4motion.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GPT4Motion&lt;/a&gt; 通过 Blender 实现了无训练，直接从文本生成视频的整个流水线。&lt;/p&gt;
&lt;p&gt;如果我们的任务是从初始帧生成视频，就可以考虑从图片重建出 3D 模型。&lt;a class=&#34;link&#34; href=&#34;https://supertan0204.github.io/physmotion_website/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysMotion&lt;/a&gt; 使用的方法就是从单张图片进行 Gaussian Splatting 的重建，然后接入物质点法进行模拟，最后经过视频模型进行渲染。如果我们的单视角重建（本质上是对其他视角的生成任务）足够好，那么生成视频的质量就有保证。但是话又说回来，我们不正是应该利用预训练视频模型的能力来帮助单视角生成的任务吗？为什么反而抛弃了大模型在这方面的能力而只把大模型作为一个渲染器呢？&lt;/p&gt;
&lt;p&gt;我们可以发现，如果只是用大模型去增强现有的图形管线，那么不可避免的需要很长的管线，并且没有充分利用大模型的能力。最理想的情况是，我们用最少的规则限制和控制信号，提供最基础的三维物理和几何的保证，其他的交给预训练模型补充细节。在这个方向上，&lt;a class=&#34;link&#34; href=&#34;https://cinemaster-dev.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CineMaster&lt;/a&gt; 是一个很有意思的尝试，只通过最简单的包围盒作为条件，就能实现很好的视频控制生成效果。&lt;/p&gt;
&lt;h3 id=&#34;推断物理信息&#34;&gt;推断物理信息
&lt;/h3&gt;&lt;p&gt;前文提到的都是在有物理信息的情况下进行仿真/生成的工作(也就是pipeline3)，如果我们希望将这一概念应用于机器人操作，那么关键问题就在于：如何估计现实世界中物体的物理参数？这是一个极具挑战性的问题，因为目前市面上很少有数据集能够提供包括物体质量、弹性系数等在内的各种物理信息。因此，依赖于大规模数据集进行学习来解决这一问题变得非常困难，特别是当数据集质量较低且规模较小时，模型的泛化能力也会受到限制。那么，如何应对这一挑战呢？如果我们选择依赖物理仿真，是否能够提升模型的泛化能力呢？毕竟，像牛顿定律等自然科学规律是普适的。然而，物理仿真对算法的精度要求非常高。基于此，一个较为可行的初步思路是将模型学习与物理仿真结合：首先通过模型学习来获取物体的初始物理属性，如质量、速度等；然后借助传统物理模拟来预测物体未来的运动规律，最后再用神经网络做微调。&lt;/p&gt;
&lt;p&gt;在这个方向，&lt;a class=&#34;link&#34; href=&#34;https://www.math.ucla.edu/~cffjiang/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;蒋陈凡夫&lt;/a&gt;他们组在此基础上做了很多相似的工作，最为出名的是&lt;a class=&#34;link&#34; href=&#34;https://sites.google.com/view/PAC-NeRF&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PAC-Nerf&lt;/a&gt;
与&lt;a class=&#34;link&#34; href=&#34;https://xpandora.github.io/PhysGaussian/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysGaussian&lt;/a&gt;。他们的思想非常朴素，但是却很有用。其整体框架如下：
&lt;img src=&#34;https://Peraspera1.github.io/images/physgaussian/methodoverview.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;代码整体框架&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在此基础上，还有两个工作，分别是&lt;a class=&#34;link&#34; href=&#34;https://physdreamer.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Physdreamer&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://dreamgaussian.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DreamGaussian&lt;/a&gt;，但是都是一些incremental的工作而且做的都是偏生成方向。&lt;a class=&#34;link&#34; href=&#34;https://physdreamer.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Physdreamer&lt;/a&gt;应该是这个领域较新的成果了，他们结合了&lt;a class=&#34;link&#34; href=&#34;https://xpandora.github.io/PhysGaussian/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysGaussian&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://dreamgaussian.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DreamGaussian&lt;/a&gt;两篇工作，简单点说就是PhysGaussian用了多帧的真实视频作为输入(监督信号)，恢复物理属性后，再去预测渲染未来的视频帧，而Physdreamer则只输入视频的第一帧，由第一帧图片生成后续的帧，并将其作为监督信号，再接入PhysGaussian的框架。缺点是显而易见的，生成的视频质量肯定没办法和真实的视频相比的，而且二者之间的误差肯定会随着时间的推移增加，所以他们只选择了前面生成的10帧左右作为监督，那么也就无法生成长视频序列了。而且他们的结果如下:
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/image3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;dreamer&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这些评分的意思，比如上面表格的第一个内容表示，对于alocosia（这是一种植物）的视频生成，有86%的人认为他们生成的视频在motion realism的指标下比真实的视频还要好，不过都是人工打分的指标，感觉没啥参考性。而且如果想用到操作的领域，用生成去做肯定是不合适的。不过他们的工作中有一个我觉得还不错的点，他们对于所有的Gaussian点做了knn下采样，也就是只对所谓的driven-particle作仿真，这一点大大加快了仿真的速度。又联想到最近的一篇文章&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2405.18133&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Grid-Free Fluid Solver based on Gaussian Spatial Representation&lt;/a&gt;，这篇工作看格式应该是要投SIGGRAPH的，他们是在PhysGaussian的基础上加速了流体的仿真，简单点说就是原本3DGS核携带的信息是球谐函数，他们把这个信息改成了物理属性，或者可以这么理解，本来3DGS渲染的是RGB，他们改成了渲染速度矢量($V_x, V_y, V_z$)。这个思路我感觉挺好的，而且可以和之前knn下采样的思路结合，我们可以在这个基础上继续加新的东西，而且他们的代码还没有开源，所以我最近有时间的话想从头复现下这篇论文，顺便学习下cuda的代码。&lt;/p&gt;
&lt;p&gt;总的来说，我个人感觉这个领域做的人比较少，而且这些工作的不足之处也非常明显，比如首先他们不能将物体的前景与背景分离开来(比如可以参考&lt;a class=&#34;link&#34; href=&#34;https://stevenlsw.github.io/physgen/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysGen&lt;/a&gt;的pipeline,并用&lt;a class=&#34;link&#34; href=&#34;https://lotus3d.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LOTUS&lt;/a&gt;做分割?)，其次他们还是只能模拟简单的物理规律，比如他们都把花朵建模为了纯粹的弹性-质点模型，而且在物理规律模拟的过程中都是比较传统的算法，这对于机器人的操作来说都是必须要解决的问题，我感觉这个领域还是值得研究的。&lt;/p&gt;
&lt;h2 id=&#34;机器人中的应用-application-in-robot&#34;&gt;机器人中的应用 Application in Robot
&lt;/h2&gt;&lt;p&gt;在机器人领域的调研中，最符合我idea的工作是这篇23年的RAL &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.09420&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DANOs&lt;/a&gt;。&lt;img src=&#34;https://Peraspera1.github.io/images/summary/image1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;概览&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;可以参考他们的&lt;a class=&#34;link&#34; href=&#34;youtu.be/Md0PM-wv_Xg&#34; &gt;视频&lt;/a&gt;作进一步的了解。
&lt;img src=&#34;https://Peraspera1.github.io/images/summary/image2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;部分视频截图&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;他们的工作是通过纯粹的视觉信息来估计物体的质量，重心，摩擦系数这些属性并辅助机械爪工作，由于是23年的工作，所以他们的框架还是建立在Nerf的基础上的，也就是先建立一个体素场，然后在体素的密度和质量之间，通过神经网络的方式建立一个映射，从而恢复出物体的属性。而且他们还在实物实验上验证了他们的结果，不过这个实物实验比较简单，就是一个肥皂在桌子上滑动，然后预测摩擦力/重力，算出物体的运动规律。&lt;/p&gt;
&lt;p&gt;那么由这个工作出发，自然地会想到两条路径。首先，目前，3DGS技术已经被广泛应用，并且相比NeRF，其基于点云的表示方式在物理仿真中更具优势。因此，直接将 NeRF替换为3DGS是一个显而易见的改进方向。这个工作一眼就能看出来能和PhysGaussian结合，虽然我没找到相关工作，但肯定有人会去做，不过别人可能是普通的二爪机械手，我们是灵巧手，而且还能加入触觉信息，这也是我觉得一个可以探索的方向。&lt;/p&gt;
&lt;p&gt;其次由这个工作，我还联想到了很多预训练上的工作，例如，在现实任务中，机械手可能难以直接抓取桌面上的一张卡片，而更合理的策略是先将卡片推到桌子的边缘，再进行夹取。本质上，这一推卡片的过程与上述预测摩擦力/重力的方法是相似的，都涉及物理信息的推理与利用。然而，据我观察，大部分机器人领域的研究仍然主要关注于抓取姿态的生成，即如何在几何层面找到最优的抓取方式，而对物体的物理属性（如摩擦、质心、变形）以及环境因素的考量较少。因此，我的研究想法正是希望从物理信息的角度出发，探索更加智能的抓取策略。（如果后续要做这个方向上的工作，我希望能从这个简单的任务（推卡片）开始做）&lt;/p&gt;
&lt;p&gt;有人可能会质疑：如果目标是执行某些运动学任务（如推卡片、打乒乓球等），为什么不直接用端到端控制器进行学习？目前已有研究通过强化学习等方法，成功训练出了能够打乒乓球或羽毛球的机器人，是否还有必要引入物理建模？&lt;/p&gt;
&lt;p&gt;首先，从直觉上来说，我觉得通过物理规律建模得到的结果可以泛化到很多未知的情境中，端到端的控制方法虽然可以在特定任务上表现出色，但它们通常是基于数据驱动的黑盒模型，容易受到环境因素（如颜色、光照变化等）的干扰。而基于物理规律的建模方法则能够泛化到更多未知场景，不依赖于特定数据分布。&lt;/p&gt;
&lt;p&gt;其次，按照我文章一开始提出的overview，哪一个环节出现了问题，对于我来说都是可控的，这种可控性使得物理建模能够更好地与新兴的研究成果结合（比如之前提到的lotus），并进行模块化优化。而端到端学习往往难以解释模型的决策逻辑，并且对于计算资源的消耗量巨大。&lt;/p&gt;
&lt;p&gt;第三，打乒乓球这种工作只能局限于运动学模型，而对于更精细的操作（如灵巧手抓取柔性物体、操纵复杂工具、物体局部形变等），物理仿真能够提供更完整的解释。例如，在精细抓取任务中，局部形变、摩擦力、微观接触点的作用不可忽视，而这些信息很难通过端到端的黑盒方法直接学习到。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Physdreamer</title>
        <link>https://Peraspera1.github.io/p/physdreamer/</link>
        <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://Peraspera1.github.io/p/physdreamer/</guid>
        <description>&lt;img src="https://Peraspera1.github.io/images/physdreamer_cover.jpg" alt="Featured image of post Physdreamer" /&gt;&lt;h1 id=&#34;论文信息&#34;&gt;论文信息
&lt;/h1&gt;&lt;h2 id=&#34;信息概览&#34;&gt;信息概览
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;ECCV 2024 Oral Presentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文题目：&lt;/strong&gt; PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文单位：&lt;/strong&gt; MIT&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;是否开源：&lt;/strong&gt; 是&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;
一种基于物理学的方法，通过利用视频生成模型学习的对象动力学先验，赋予静态3D对象交互式动力学，也就是使静态3D对象能够以物理上似乎合理的方式动态响应交互刺激。&lt;/p&gt;
&lt;p&gt;该方法使用3D高斯粒子表示物体，使用神经场建模材料属性，并通过可微分仿真（使用材料点法，MPM）模拟动态。&lt;/p&gt;
&lt;h2 id=&#34;论文思路&#34;&gt;论文思路
&lt;/h2&gt;&lt;p&gt;问题：给一张图片，比如一朵花，想知道这朵花在微风吹过后的动态信息，也就是求一个物体对于新物理交互的响应。&lt;/p&gt;
&lt;p&gt;但是求解这个响应，需要对物体的性质有较为准确的估计（比如两种弹性系数不同的弹簧，对其施加相同大小和方向的力，其变形显然是不一样的）。&lt;/p&gt;
&lt;p&gt;而这个性质是很难测量的，或者说难以形成大规模的数据集以供学习。&lt;/p&gt;
&lt;p&gt;但是人类能从观察物理世界和与物理世界互动中获得的物理先验知识，受此启发，作者从大量的视频先验中学习动力学先验，&lt;/p&gt;
&lt;p&gt;为了简化，这篇文章只对弹性物体做了仿真，那么估计的物理属性，有质量、杨氏模量和泊松比。质量等于密度乘体积，论文中粒子的体积是体素的体积除以其中包含的粒子数，密度是给定的常数，泊松比也是给定的常数，所以最后优化的只是一个杨氏模量的场。&lt;/p&gt;
&lt;p&gt;论文的关键思想是生成运动中物体的合理demo，比如一朵花，他把花离散为很多稠密的点，但这些点不是同构的，因此每个点的杨氏模量都不一样，然后按照物理属性去优化材质场E以匹配这个合成的运动。
我们首先从某个视点为 3D 场景出发渲染静态图像。然后，我们利用图像到视频模型(SVD)生成一个短视频剪辑 {$I_0$， $I_1$， . . . ， $I_T$ }，描绘对象的真实运动，这个生成的模型是GT来监督模拟得到的图像，然后再通过可微分模拟和可微渲染来优化材料场E(x)和初始速度场$v_0$(x)，使得模拟的渲染视频与生成的视频匹配。
但其实我觉得核心的部分还是图中下面的箭头，也就是PhysGaussian的工作比较重要。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Peraspera1.github.io/images/physdreamer/image1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;论文整体框架&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;细节部分&#34;&gt;细节部分
&lt;/h2&gt;&lt;p&gt;仿照PhysGaussian内部填充？&lt;/p&gt;
$$
\rho \frac{D v}{D t} = \nabla \cdot \sigma + f, \frac{D \rho}{D t} + \rho \nabla \cdot v = 0
$$&lt;p&gt;v 是欧拉视角，密度是常量，f是外力。&lt;/p&gt;
&lt;p&gt;MPM的实现细节需要单独花时间细看。&lt;/p&gt;
$$
x^{t+1}, v^{t+1}, F^{t+1}, C^{t+1} = S(x^{t}, v^{t}, F^{t}, C^{t}, \theta , \Delta t)
$$&lt;p&gt;F和C分别是局部变形场的梯度和应力场的梯度，$\theta$ 代表所有的物理量，在文章里代表E，$\Delta \approx 1 \times 10^{-4}$，仿真了100步。&lt;/p&gt;
$$
\hat{I}^t = F_{render}(x^t, \alpha, R^t, \Sigma, c)
$$&lt;p&gt;R代表所有粒子的旋转矩阵，&lt;/p&gt;
$$
L^t = \lambda L_1(\hat{I}^t, I^t) + (1-\lambda)L_{D-SSIM}(\hat{I}^t, I^t)
$$&lt;h2 id=&#34;创新点mpm加速&#34;&gt;创新点(MPM加速)
&lt;/h2&gt;&lt;p&gt;高斯模型包含成千上万个点，这对于模拟来说效率较低。因此，本文采用了下采样方法，每个下采样后的点能够有效描述其对应领域的信息。此外，下采样对3D几何形状（3DGS）的表征同样至关重要。因为3DGS表征存在过于局部化的问题（不同区域之间的表征可能会出现突变或不连贯），这会导致空间表征的不连续性。通过下采样后，每个点包含了其领域的信息，从而有可能推动表征向混合高斯模型（mixture-Gaussian）方向发展，使得空间的整体表示更加连续。这样的方法可能为将三维场景表示为一串序列提供了思路，可以进一步应用于MLLM。例如，可以将该序列视作一个Encoder-Decoder模型，并通过重建信息作为监督信号进行训练。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Peraspera1.github.io/images/physdreamer/image2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;mpm加速&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果
&lt;/h2&gt;&lt;p&gt;数据集：八个真实场景，大部分是花，这个作为对照组&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Peraspera1.github.io/images/physdreamer/image4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;视频生成&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其实没什么意义，因为本身这篇论文是在前两篇的基础上做的，而且PhysGaussian没有优化物理参数，DreamGaussian没有物理假设。。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论
&lt;/h2&gt;&lt;h3 id=&#34;视频生成的方式&#34;&gt;视频生成的方式
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Peraspera1.github.io/images/physdreamer/image3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;视频生成&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;用SVD采样得到了14帧的信息作为监督。&lt;/p&gt;
&lt;p&gt;疑问的点，text-prompt怎么设计，比如花在空中摇摆，或者被人碰了一下，怎么去量化这个幅度？&lt;/p&gt;
&lt;h3 id=&#34;loss的设计&#34;&gt;loss的设计
&lt;/h3&gt;&lt;p&gt;用生成得到的视频监督是否合理？
因为整个3DGS的参数很多，这篇文章只是监督了E，而其他的位置，速度等信息都是仿真算出来的，所以DoF，或者说优化的参数空间其实比较小。但是如果要学习更多的物理信息，只用SVD去监督肯定不合理。&lt;/p&gt;
&lt;p&gt;其次，生成得到的视频离真实场景还是有差别，所以还是要做一个trade-off，一种方法是减少生成视频对模型的影响，例如使用结构损失作为损失函数，或者将生成的视频帧作为guidance来进行distillation，另一种是降低估计的Dof，想这篇文章做的那样，固定泊松比和质量，只估计杨氏模量，第三种方式是提高视频生成的能力，脱离SDS损失函数的监督，转向全监督学习，即让生成的视频与真实场景之间有更多直接的监督。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Generative Image Dynamics</title>
        <link>https://Peraspera1.github.io/p/generativeid/</link>
        <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
        
        <guid>https://Peraspera1.github.io/p/generativeid/</guid>
        <description>&lt;img src="https://Peraspera1.github.io/post/GID/GIDfront.jpg" alt="Featured image of post Generative Image Dynamics" /&gt;&lt;h1 id=&#34;论文信息&#34;&gt;论文信息
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;CVPR 2024 Best Paper&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文题目：&lt;/strong&gt; Generative Image Dynamics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;是否开源：&lt;/strong&gt; 否&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一种从静态图像生成动态运动的方法，特别是针对自然的振荡运动，如树木在风中摇动、花朵或蜡烛火焰的摆动等。
核心思想是使用频谱体积，这是一种基于频率的像素运动表示，通过真实视频序列进行学习。&lt;/p&gt;
&lt;h1 id=&#34;细节&#34;&gt;细节
&lt;/h1&gt;&lt;p&gt;输入单张图片$I_0$, 输出${\hat{I}_1, \hat{I}_2, ..., \hat{I}_T}$，&lt;/p&gt;
&lt;p&gt;用LDM从输入的单张图片中预测谱体积
$\mathcal{S}=\left(S_{f_0},S_{f_1},...,S_{f_{K-1}}\right)$
，然后再利用这个谱体积恢复$\mathcal{F}=(F_1,F_2,...,F_T)$，即后面T个时刻的运动。$F_t(p)$ 代表t时刻$I_0$中第p个像素的位置。（$I_t^\prime(\mathbf{p}+F_t(\mathbf{p}))=I_0(\mathbf{p})$）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spectral volume（谱体积）：&lt;/strong&gt; 从视频中提取的每像素轨迹的时间傅里叶变换。&lt;/p&gt;
&lt;p&gt;频谱分析-&amp;gt;解决生成视频的长期时间一致性??&lt;/p&gt;
&lt;p&gt;k个频率，(x,y)+- 共4K个channel&lt;/p&gt;
&lt;p&gt;对于一张 $H \times W$的图片，每个像素p可以表示为$I_{t}(p) = \Sigma_{k} [Asin(k \omega x t) + B cos(k \omega x t)]$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;值得阅读的参考论文&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Myers Abraham Davis. Visual vibration analysis. PhD thesis, Massachusetts Institute of Technology, 2016.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
