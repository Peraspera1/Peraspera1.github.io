[{"content":"论文信息 信息概览 ECCV 2024\n论文题目： DoughNet: A Visual Predictive Model for Topological Manipulation of Deformable Objects\n论文单位： Columbia University\n是否开源： 是\n总结： 纯视觉的预测模型，主要用于长时间预测可变性物体(如黏土/橡皮泥)的在不同物理条件下的几何拓扑变换。 可变形物体（如面团）的操作通常涉及拓扑变化（如分裂、合并）。传统方法主要关注几何形变，而忽略了拓扑变化。为了解决这个问题，研究者提出 DoughNet，一个基于 Transformer 的视觉预测模型，能够推测因不同工具或操作方式导致的拓扑变化。\n论文细节 输入输出 DoughNet 需要三个主要输入：\n初始状态的 RGB-D 图像：通过深度相机（如RealSense）获取点云数据。 由于是单视角，数据是不完整的，因此需要进一步补全。\n机器人端执行工具（End-Effector, EE）的几何形状： EE 可以是剪刀、夹子、滚轴等工具，每种工具会对物体施加不同影响。 该工具的几何形状以点云形式输入。\n操作轨迹（Action Trajectory）：机器人执行的操作，包括工具的移动路径、开合角度、力的大小等。\nDoughNet通过潜在空间预测生成两个核心输出：\n1 几何预测（Geometrical Prediction）：\n预测物体的形状变化（如被拉长、压扁、弯曲等）。 结果以点云、体素或隐式表面表示（Occupancy Map） 输出。\n2 拓扑预测（Topological Prediction）：\n预测物体是否会合并、分裂或变形为不同拓扑结构（如从球变为环）。 结果以拓扑图（Topology Graph）形式表示，包括：连通组件数量（Number of Components），每个组件的拓扑属性（如 genus, 环数）。 这些输出最终用于机器人操作规划，帮助选择合适的工具和动作策略。\nfor example,输入：初始面团形态（RGB-D 图像）;给定机器人工具为刀片；再给一段向下切割的轨迹。 那么这个网络会预测切割过程中的形态变化（面团从整体到分裂）及最终分裂后的面团形态（两个独立部分）\n网络信息流 编码 DoughNet采用去噪自编码器来处理输入数据，并生成潜在表示（Latent Codes）。\n首先对于物体的初始点云数据 $X \\in ℝ^{N×(3+1)}$（XYZ 位置 + 深度值）。使用Transformer Cross-Attention 计算点云之间的关系，并生成一组潜在特征（Latent Codes）。 再使用自注意力机制（Self-Attention）在点云上执行全局聚合，生成一个紧凑的潜在编码（Z）。这个潜在编码允许 DoughNet 处理不同大小和拓扑结构的物体。\n然后把EE（工具）的几何信息也被编码成潜在向量，与物体的潜在表示一起处理，以推测它们的交互方式。\n输出：一组潜在编码 [z]，表示物体当前的形状及拓扑信息。\n预测 DoughNet采用自回归预测来模拟物体在操作过程中的变化。\n输入：\n物体当前的潜在编码 [zt]。\nEE 的编码 [zt_m]，代表操作工具的信息。\n当前的操作 [a_t]（如 EE 移动路径）。\nTransformer 预测:\n通过Cross-Attention，模型推测物体与 EE 交互后下一步的潜在编码 [z_{t+1}]。 由于预测发生在潜在空间（Latent Space），计算量较低，且模型可以学习更稳定的特征。\n多步预测（Multi-Step Prediction）：\n由于是自回归结构，DoughNet可以递归地预测未来形态（反复输入自己的输出，进行多步预测）。\n预测的输出是： 1 下一步物体的潜在编码 [z_{t+1}]。 2 EE 的影响 [z_{t+1}^m]（用于判断 EE 选择是否合理）。\n解码 DoughNet 需要将预测的潜在编码转换回物体的几何形状和拓扑结构。\n形状解码：采用 Transformer 解码层，将潜在编码 [z] 还原成 物体表面的点云或体素网格。\n组件分割（Component Segmentation）：预测哪些点属于哪个物体组件（如两个面团是否仍然是一个整体，或已经分裂成两部分）。\n拓扑预测（Topology Prediction）： 预测物体的 拓扑结构（genus, 组件数），比如：是否分裂？是否合并？是否变成一个环？ 采用Cross-Attention计算物体在不同时间步的拓扑关系。\n最终输出：完整物体的形状（Occupancy Map, Point Cloud, Mesh）和拓扑结构。\n","date":"2025-03-06T10:51:30+08:00","image":"https://Peraspera1.github.io/images/doughs/cover.png","permalink":"https://Peraspera1.github.io/p/dough/","title":"Dough"},{"content":"Cosmos+ComfyUI 环境配置记录 配置ComfyUI 环境 官方推荐的是python=3.12版本，但是由于我的cuda是11.7版本，没有对应的pytorch，所以我改成了3.11版本的python及其对应的pytorch，这里的版本可根据需要自行修改。\n在终端中输入：\n1 2 3 4 5 6 7 conda create --name comfyui python=3.11 conda activate comfyui conda install pytorch==2.5.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=12.1 -c pytorch -c nvidia git clone https://github.com/comfyanonymous/ComfyUI.git cd ComfyUI/ pip install -r requirements.txt -i https://pypi.mirrors.ustc.edu.cn/simple/ 之后运行\n1 python main.py 出现如下结果： 打开网站后结果如下 至此comfyUI配置完成\nCosmos 相关配置 下载 语言模型 并保存到ComfyUI/models/text_encoders/ 路径下\n下载 vae 并保存到 ComfyUI/models/vae/ 路径下\n下载 cosmos模型 并保存到 ComfyUI/models/diffusion_models 路径下\nComfyUI 工作流配置 在官方文档中，下载对应的json文件并保存到comfyui下的workflow的文件夹（自己新建），下图以text2video为例\n然后在网页端打开这个工作流直接运行就可以了\n踩坑 Q1\n1 module \u0026#39;torch\u0026#39; has no attribute \u0026#39;float8_e4m3fn\u0026#39; A1\ntorch版本太低了，升级到pytorch=2.5.1+cuda=12.1就好\nComfyui 基本使用方法 安装了两个插件还挺好用的，分别是AlekPet和VHS\n","date":"2025-03-04T16:22:05+08:00","image":"https://Peraspera1.github.io/images/cosmos/cover.png","permalink":"https://Peraspera1.github.io/p/cosmos/","title":"Cosmos"},{"content":"论文信息 信息概览 RAL 2023\n论文题目： Differentiable Physics Simulation of Dynamics-Augmented Neural Objects\n论文单位： MIT\n是否开源： 否\n总结： 输入未经处理的rgb视频，使用可微分物理引擎来模拟其在施加的力和扭矩下的运动\n论文细节 1 估计物体质量、质心、惯性矩阵和表面摩擦系数，通过表面接触的概率?\n总体思想如下：\n他们主要的创新点还是在提取物体表面上做的，但该方法是建立在nerf场上的，没必要借鉴，在3DGS的情况下应该有更好的表示方式。 总之，这个工作主要还是建立在dojo这个他们MIT自己研究的仿真平台上的，不一定有泛用性，而且这个平台已经停止开发了。 可以去研究下genesis是怎么工作的。\n相关论文 “Marching cubes: A high resolution 3d surface construction algorithm” siggraph 生成物体表面的网格模拟运动\n","date":"2025-03-03T10:04:23+08:00","image":"https://Peraspera1.github.io/images/DANOs/cover.png","permalink":"https://Peraspera1.github.io/p/danos/","title":"DANOs"},{"content":"尽管当前的视频生成模型已能产生令人满意的效果，但生成的结果中仍常出现不自然的现象(Sora)，这些现象往往违背了我们对几何和物理常识的理解，因此近年来，许多研究尝试将物理规律融入视频生成模型来得到更好的结果。与此同时，在机器人领域，大多数研究仍主要聚焦于机器人自身的运动轨迹生成，而对环境及交互物体的感知仍停留在视觉信息的层面，未能充分利用物理规律。因此，这篇文章总结了我对近期工作的一些总结，以及一点点对于这些工作在灵巧手操作上可能应用的思考。\n我的想法大致如下： 整个工作的pipeline如下：\n给定一个任务，利用LLM预测我们需要知道哪些物理属性（比如物体的摩擦系数）； 输入一段视频作为监督信号，从该视频的初始帧开始进行物理仿真，优化物理参数场； 得到较为精确的物理参数后，生成/仿真得到该物体在被施加一个力后的响应； 在已有的手物交互轨迹基础上，如果能准确预测物体对外力的响应，就能进一步微调手的施力方向和大小，或者将这些信息直接融入轨迹生成过程中，以提升交互精度。 一句话概括就是让机器人理解真实环境中的物理属性，从而更好地辅助操作实现。那么自然而然地会引出两个问题，\n首先，如何从环境中恢复物体的物理属性并进行物理仿真？\n其次，在机器人已知物体的物理属性后，如何利用这些信息优化灵巧手的操作？\nPhysics Simulation \u0026amp; Video Generation 事实上，给定视频恢复物理属性(pipeline2)与给定物理属性生成视频(pipeline3)互为反问题(不过我更关注的是生成物体对于外力的响应，并不需要那种精细的视频)。然而，近年来学界的研究更倾向于后者，这一领域的研究主要分为以下三个方向：\n物理对齐 Physics Alignment 在语言模型中，对齐（Alignment）指的是通过一系列算法和工程手段，修正模型的行为，使其输出符合预设的安全边界和人类意图。与之对应的，修正视频模型的输出使其满足物理规则的过程就是物理对齐。作为对齐领域的代表性工作，InstructGPT 提出了两种对齐方法：\n监督微调（Supervised Fine Tuning，SFT）人工标注高质量的提示（prompt）和回答（output）数据集，通过监督学习的方式微调模型。\n基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）对同一个提示，模型输出多个回答，人工对这些回答进行比较打分；使用打分的结果训练一个反馈模型（Reward Model），用于评价模型输出的好坏；使用反馈模型对模型进行强化学习，比如近端策略优化（Proximal Policy Gradient，PPO）。\n将这两种想法用于视频模型中是比较直观的。对于 SFT，我们就需要使用物理真实的视频作为输入。但是真实世界中的视频当然都是物理真实的，可能的问题是视频动态不够，导致模型没有接受到足够的动态信息。因此像 Cosmos 在预训练阶段就会保证数据能够反映真实物理规则。这主要是通过两点做到的：\n收集包含大量动态的视频：包括驾驶视频、手部动作、第一人称视角、模拟结果等等。 对数据进行过滤：剔除质量低的、缺乏动态的、非物理等的视频，并提取一部分高质量视频作为后训练数据集。 尽管 Cosmos 的论文在 5.3.2 节专门讨论了物理对齐的问题，但是实际上并没有做更多的尝试，只是在几个场景中测试了 Cosmos 生成的结果是否吻合模拟/真实的物理。\n对于 RLHF 而言，首要问题是需要一个反馈模型来判别模型输出的结果是否满足物理规律。这方向一个代表工作是 VideoPhy。这篇工作的核心是对市面上十二个视频模型的生成结果进行人工打分，然后训练一个打分网络 VideoCon-Physics。打分分为两个维度，每个维度得分只有 0 或 1：一个是语义的符合程度（Semantic Adherence，SA），一个是是否符合物理常识（Physical Commonsense，PC），结果如下： Benchmark 可以发现成绩最好的是开源模型 CogVideoX-5B，但是也只是勉强及格的水平。这方向类似的工作还有 VideoScore，PhyGenBench。理论上有了打分模型之后我们就可以对视频模型进行强化学习对齐了，OnlineVPO 就使用了 VideoScore 作为反馈模型微调了 OpenSora 模型，使其在 VideoScore 得分上超越了其他模型。\n整体上来说，物理对齐比较依赖预训练大模型的能力。对于语言模型来说，对齐往往会降低模型在基准测试上的分数，称为支付对齐税（Alignment Tax）。对于视频模型情况应该是类似的，增强其在物理动态方面的能力可能导致其他能力的削弱。因此，一个更本质的问题是，通过预训练的方式大模型是否能够足够泛化地学到物理规律？字节的工作 How Far is Video Generation from World Model? A Physical Law Perspective 是这个方向的一个初步探索。\n二维平面模拟 2D Physics Ok，如果视频模型短期内无法达到我们对于物理规律的需求，那我们是否可以通过加入物理模拟的方式增强这方面的能力呢？由于视频模型都是 2D 的，我们可以先从二维平面上的模拟开始，这一领域分为两个方向，一个是显式地加入物理规律并在屏幕空间上进行模拟，另一个则是隐式地推断物体的动力学信息。\n屏幕空间模拟（Screen-space Simulation）指的是绕过 3D 模型，直接在屏幕空间中模拟物体的动态。这方面的一个代表性工作是 PhysGen。 Physgen 流程图 大致的流程分为三步：1. 给定一张初始图片，我们先做分割，并使用图像理解模型获取法向贴图、语义等信息，然后使用大语言模型推测对应的材质；2. 使用屏幕空间的 2D 模拟器进行模拟（刚体模拟）；3. 使用视频模型将模拟结果与法向贴图等整合起来，得到最终视频。可以发现，这个流程中其实并不需要大模型生成动态，大模型提供的是一个满足时间连续性的\u0026quot;渲染器\u0026quot;，将模拟结果渲染成视频。最近的 PhysAnimator 也是这个思路，不过将模拟的对象进一步扩展到了布料这样的软性材料。\n当然，也有不显式加入物理模拟的方式来实现图片上的动力学的工作，而是通过分析图片中物体的振动规律得到信息，并利用这些信息来推断物体的物理特性和驱动它们运动的力，这一工作最早可以追溯到Davis的博士论文Visual vibration analysis。如果有了物体对于一个已知力的响应，那么我们是不是就可以在不知道物理规律的前提下分析一个物体对于未知的力的响应了呢（类似于反馈-控制，黑盒模型）？2024年的CVPR Best Paper Generative Image Dynamics 就做了这样的一件事。与PhysGen不同，这篇论文并没有显式地建模出物体的运动规律，而是只学习到了振动频率(事实上，物体振动频率满足该规律：$\\omega = \\sqrt{\\frac{k}{m}}$)，即学习每个像素点的振动规律(利用傅里叶变换分解，学习的参数是傅里叶的系数)。在不考虑训练成本的情况下，相比于显式地模拟，这种方式的推理速度肯定更快，但是显然只能局限于振动这种简单的物理规律，当然，通过改变基函数的方式（比如把傅里叶级数换成勒让德级数）也许可以将该方法推广到更多真实情景，这也是一个值得探索的方向。\n总的来说，在二维平面上的模拟的优点很明显，我们能够对生成的结果进行非常精确的控制，并且在小幅度内基本满足我们对物理规则的认知。但是缺点同样很明显，由于我们是在屏幕空间做分割和模拟，我们永远只能生成物体一面的结果，像布料的遮挡褶皱也无法处理。并且，生成结果的视角只能是固定的。这使得这类方法只适用于生成动态壁纸这样比较受限的应用，不能作为通用的视频生成方法。\n不过我觉得PhysGen的思维范式很好，如果要继续往下做的话我更倾向于在该工作上的框架上去做。\n三维空间模拟 3D Physics 既然 2D 的模拟终究是妥协，那不如我们直接回到三维模拟。回顾传统的图形管线，生成视频的过程大致可以分为准备3D 资产、进行模拟、渲染结果这三步。这三步中预训练的视频模型可以充当一个非常好的渲染器，比如下图展示的，Cosmos 可以将三维模拟的结果风格迁移到真实场景的视频。 Cosmos中的风格迁移 在有 3D 资产和物理参数的基础上，模拟也不是一个困难的问题。传统模拟算法在平衡模拟效果和速度上已经提供了非常多的选择。因此最大的问题在于第一步：对于用户给定的一个语言提示，或者是初始帧，如何获取对应的 3D 资产。对应这两种情况我们可以看到两种解决方法，一是训练文本生成 3D 资产的模型，二是从真实图片中重建。\n首先，世面上已经有很多专注做文本生成 3D 资产的 AI，比如 Rodin，Meshy 等,可以直接将这些模型导入到像 Houdini、Blender 这样的图形软件中进行模拟。之前受到很多关注的 Genesis 想做的就是这个思路。另一方面，过程建模（Procedural Modeling）使用形式语言或者节点化的方式描述模型的生成过程，可以将 3D 模型与文本直接联系起来。比如 SVG 图片使用 html 标记语言，CAD 模型可以完全用代码表示，Houdini 用节点系统描述模型等等。在有了代码化的描述之后，我们就可以通过语言模型去生成这些代码，也就生成了 3D 模型。Infinigen 通过 Blender 构建了描述自然和室内场景的过程建模语言，因此可以实现文本生成三维场景。GPT4Motion 通过 Blender 实现了无训练，直接从文本生成视频的整个流水线。\n如果我们的任务是从初始帧生成视频，就可以考虑从图片重建出 3D 模型。PhysMotion 使用的方法就是从单张图片进行 Gaussian Splatting 的重建，然后接入物质点法进行模拟，最后经过视频模型进行渲染。如果我们的单视角重建（本质上是对其他视角的生成任务）足够好，那么生成视频的质量就有保证。但是话又说回来，我们不正是应该利用预训练视频模型的能力来帮助单视角生成的任务吗？为什么反而抛弃了大模型在这方面的能力而只把大模型作为一个渲染器呢？\n我们可以发现，如果只是用大模型去增强现有的图形管线，那么不可避免的需要很长的管线，并且没有充分利用大模型的能力。最理想的情况是，我们用最少的规则限制和控制信号，提供最基础的三维物理和几何的保证，其他的交给预训练模型补充细节。在这个方向上，CineMaster 是一个很有意思的尝试，只通过最简单的包围盒作为条件，就能实现很好的视频控制生成效果。\n推断物理信息 前文提到的都是在有物理信息的情况下进行仿真/生成的工作(也就是pipeline3)，如果我们希望将这一概念应用于机器人操作，那么关键问题就在于：如何估计现实世界中物体的物理参数？这是一个极具挑战性的问题，因为目前市面上很少有数据集能够提供包括物体质量、弹性系数等在内的各种物理信息。因此，依赖于大规模数据集进行学习来解决这一问题变得非常困难，特别是当数据集质量较低且规模较小时，模型的泛化能力也会受到限制。那么，如何应对这一挑战呢？如果我们选择依赖物理仿真，是否能够提升模型的泛化能力呢？毕竟，像牛顿定律等自然科学规律是普适的。然而，物理仿真对算法的精度要求非常高。基于此，一个较为可行的初步思路是将模型学习与物理仿真结合：首先通过模型学习来获取物体的初始物理属性，如质量、速度等；然后借助传统物理模拟来预测物体未来的运动规律，最后再用神经网络做微调。\n在这个方向，蒋陈凡夫他们组在此基础上做了很多相似的工作，最为出名的是PAC-Nerf 与PhysGaussian。他们的思想非常朴素，但是却很有用。其整体框架如下： 在此基础上，还有两个工作，分别是Physdreamer和DreamGaussian，但是都是一些incremental的工作而且做的都是偏生成方向。Physdreamer应该是这个领域较新的成果了，他们结合了PhysGaussian和DreamGaussian两篇工作，简单点说就是PhysGaussian用了多帧的真实视频作为输入(监督信号)，恢复物理属性后，再去预测渲染未来的视频帧，而Physdreamer则只输入视频的第一帧，由第一帧图片生成后续的帧，并将其作为监督信号，再接入PhysGaussian的框架。缺点是显而易见的，生成的视频质量肯定没办法和真实的视频相比的，而且二者之间的误差肯定会随着时间的推移增加，所以他们只选择了前面生成的10帧左右作为监督，那么也就无法生成长视频序列了。而且他们的结果如下: 这些评分的意思，比如上面表格的第一个内容表示，对于alocosia（这是一种植物）的视频生成，有86%的人认为他们生成的视频在motion realism的指标下比真实的视频还要好，不过都是人工打分的指标，感觉没啥参考性。而且如果想用到操作的领域，用生成去做肯定是不合适的。不过他们的工作中有一个我觉得还不错的点，他们对于所有的Gaussian点做了knn下采样，也就是只对所谓的driven-particle作仿真，这一点大大加快了仿真的速度。又联想到最近的一篇文章A Grid-Free Fluid Solver based on Gaussian Spatial Representation，这篇工作看格式应该是要投SIGGRAPH的，他们是在PhysGaussian的基础上加速了流体的仿真，简单点说就是原本3DGS核携带的信息是球谐函数，他们把这个信息改成了物理属性，或者可以这么理解，本来3DGS渲染的是RGB，他们改成了渲染速度矢量($V_x, V_y, V_z$)。这个思路我感觉挺好的，而且可以和之前knn下采样的思路结合，我们可以在这个基础上继续加新的东西，而且他们的代码还没有开源，所以我最近有时间的话想从头复现下这篇论文，顺便学习下cuda的代码。\n总的来说，我个人感觉这个领域做的人比较少，而且这些工作的不足之处也非常明显，比如首先他们不能将物体的前景与背景分离开来(比如可以参考PhysGen的pipeline,并用LOTUS做分割?)，其次他们还是只能模拟简单的物理规律，比如他们都把花朵建模为了纯粹的弹性-质点模型，而且在物理规律模拟的过程中都是比较传统的算法，这对于机器人的操作来说都是必须要解决的问题，我感觉这个领域还是值得研究的。\n机器人中的应用 Application in Robot 在机器人领域的调研中，最符合我idea的工作是这篇23年的RAL DANOs。\n可以参考他们的视频作进一步的了解。 他们的工作是通过纯粹的视觉信息来估计物体的质量，重心，摩擦系数这些属性并辅助机械爪工作，由于是23年的工作，所以他们的框架还是建立在Nerf的基础上的，也就是先建立一个体素场，然后在体素的密度和质量之间，通过神经网络的方式建立一个映射，从而恢复出物体的属性。而且他们还在实物实验上验证了他们的结果，不过这个实物实验比较简单，就是一个肥皂在桌子上滑动，然后预测摩擦力/重力，算出物体的运动规律。\n那么由这个工作出发，自然地会想到两条路径。首先，目前，3DGS技术已经被广泛应用，并且相比NeRF，其基于点云的表示方式在物理仿真中更具优势。因此，直接将 NeRF替换为3DGS是一个显而易见的改进方向。这个工作一眼就能看出来能和PhysGaussian结合，虽然我没找到相关工作，但肯定有人会去做，不过别人可能是普通的二爪机械手，我们是灵巧手，而且还能加入触觉信息，这也是我觉得一个可以探索的方向。\n其次由这个工作，我还联想到了很多预训练上的工作，例如，在现实任务中，机械手可能难以直接抓取桌面上的一张卡片，而更合理的策略是先将卡片推到桌子的边缘，再进行夹取。本质上，这一推卡片的过程与上述预测摩擦力/重力的方法是相似的，都涉及物理信息的推理与利用。然而，据我观察，大部分机器人领域的研究仍然主要关注于抓取姿态的生成，即如何在几何层面找到最优的抓取方式，而对物体的物理属性（如摩擦、质心、变形）以及环境因素的考量较少。因此，我的研究想法正是希望从物理信息的角度出发，探索更加智能的抓取策略。（如果后续要做这个方向上的工作，我希望能从这个简单的任务（推卡片）开始做）\n有人可能会质疑：如果目标是执行某些运动学任务（如推卡片、打乒乓球等），为什么不直接用端到端控制器进行学习？目前已有研究通过强化学习等方法，成功训练出了能够打乒乓球或羽毛球的机器人，是否还有必要引入物理建模？\n首先，从直觉上来说，我觉得通过物理规律建模得到的结果可以泛化到很多未知的情境中，端到端的控制方法虽然可以在特定任务上表现出色，但它们通常是基于数据驱动的黑盒模型，容易受到环境因素（如颜色、光照变化等）的干扰。而基于物理规律的建模方法则能够泛化到更多未知场景，不依赖于特定数据分布。\n其次，按照我文章一开始提出的overview，哪一个环节出现了问题，对于我来说都是可控的，这种可控性使得物理建模能够更好地与新兴的研究成果结合（比如之前提到的lotus），并进行模块化优化。而端到端学习往往难以解释模型的决策逻辑，并且对于计算资源的消耗量巨大。\n第三，打乒乓球这种工作只能局限于运动学模型，而对于更精细的操作（如灵巧手抓取柔性物体、操纵复杂工具、物体局部形变等），物理仿真能够提供更完整的解释。例如，在精细抓取任务中，局部形变、摩擦力、微观接触点的作用不可忽视，而这些信息很难通过端到端的黑盒方法直接学习到。\n","date":"2025-02-27T12:49:57+08:00","image":"https://Peraspera1.github.io/images/summary_cover.png","permalink":"https://Peraspera1.github.io/p/phys_summary/","title":"Phys_Summary"},{"content":"全局修改 在 /assets/scss/custom.scss 中加入以下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 // 页面基本配色 :root { // 全局顶部边距 --main-top-padding: 30px; // 全局卡片圆角 --card-border-radius: 25px; // 标签云卡片圆角 --tag-border-radius: 8px; // 卡片间距 --section-separation: 40px; // 全局字体大小 --article-font-size: 1.8rem; // 行内代码背景色 --code-background-color: #f8f8f8; // 行内代码前景色 --code-text-color: #e96900; // 暗色模式下样式 \u0026amp;[data-scheme=\u0026#34;dark\u0026#34;] { // 行内代码背景色 --code-background-color: #ff6d1b17; // 行内代码前景色 --code-text-color: #e96900; } } //------------------------------------------------------ // 修复引用块内容窄页面显示问题 a { word-break: break-all; } code { word-break: break-all; } //--------------------------------------------------- // 文章内容图片圆角阴影 .article-page .main-article .article-content { img { max-width: 96% !important; height: auto !important; border-radius: 8px; } } //------------------------------------------------ // 文章内容引用块样式 .article-content { blockquote { border-left: 6px solid #358b9a1f !important; background: #3a97431f; } } // --------------------------------------- // 代码块基础样式修改 .highlight { max-width: 102% !important; background-color: var(--pre-background-color); padding: var(--card-padding); position: relative; border-radius: 20px; margin-left: -7px !important; margin-right: -12px; box-shadow: var(--shadow-l1) !important; \u0026amp;:hover { .copyCodeButton { opacity: 1; } } // keep Codeblocks LTR [dir=\u0026#34;rtl\u0026#34;] \u0026amp; { direction: ltr; } pre { margin: initial; padding: 0; margin: 0; width: auto; } } // light模式下的代码块样式调整 [data-scheme=\u0026#34;light\u0026#34;] .article-content .highlight { background-color: #fff9f3; } [data-scheme=\u0026#34;light\u0026#34;] .chroma { color: #ff6f00; background-color: #fff9f3cc; } //------------------------------------------- // 设置选中字体的区域背景颜色 //修改选中颜色 ::selection { color: #fff; background: #34495e; } a { text-decoration: none; color: var(--accent-color); \u0026amp;:hover { color: var(--accent-color-darker); } \u0026amp;.link { color: #4288b9ad; font-weight: 600; padding: 0 2px; text-decoration: none; cursor: pointer; \u0026amp;:hover { text-decoration: underline; } } } //------------------------------------------------- //文章封面高度更改 .article-list article .article-image img { width: 100%; height: 150px; object-fit: cover; @include respond(md) { height: 200px; } @include respond(xl) { height: 305px; } } //--------------------------------------------------- // 全局页面布局间距调整 .main-container { min-height: 100vh; align-items: flex-start; padding: 0 15px; gap: var(--section-separation); padding-top: var(--main-top-padding); @include respond(md) { padding: 0 37px; } } //-------------------------------------------------- //页面三栏宽度调整 .container { margin-left: auto; margin-right: auto; .left-sidebar { order: -3; max-width: var(--left-sidebar-max-width); } .right-sidebar { order: -1; max-width: var(--right-sidebar-max-width); /// Display right sidebar when min-width: lg @include respond(lg) { display: flex; } } \u0026amp;.extended { @include respond(md) { max-width: 1024px; --left-sidebar-max-width: 25%; --right-sidebar-max-width: 22% !important; } @include respond(lg) { max-width: 1280px; --left-sidebar-max-width: 20%; --right-sidebar-max-width: 30%; } @include respond(xl) { max-width: 1453px; //1536px; --left-sidebar-max-width: 15%; --right-sidebar-max-width: 25%; } } \u0026amp;.compact { @include respond(md) { --left-sidebar-max-width: 25%; max-width: 768px; } @include respond(lg) { max-width: 1024px; --left-sidebar-max-width: 20%; } @include respond(xl) { max-width: 1280px; } } } //------------------------------------------------------- //全局页面小图片样式微调 .article-list--compact article .article-image img { width: var(--image-size); height: var(--image-size); object-fit: cover; border-radius: 17%; } 总字数统计 在layouts/partials/footer/footer.html里增加以下代码\n1 2 3 4 5 6 7 8 9 \u0026lt;!-- Add total page and word count time --\u0026gt; \u0026lt;section class=\u0026#34;totalcount\u0026#34;\u0026gt; {{$scratch := newScratch}} {{ range (where .Site.Pages \u0026#34;Kind\u0026#34; \u0026#34;page\u0026#34; )}} {{$scratch.Add \u0026#34;total\u0026#34; .WordCount}} {{ end }} 发表了{{ len (where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34;) }}篇文章 · 总计{{ div ($scratch.Get \u0026#34;total\u0026#34;) 1000.0 | lang.FormatNumber 2 }}k字 \u0026lt;/section\u0026gt; 在assets/scss/partials/footer.scss里修改风格：\n1 2 3 4 5 .totalcount { color: var(--card-text-color-secondary); font-weight: normal; margin-bottom: 5px; } 代码块装饰 1 准备一张喜欢的风格的图片(svg格式)放到static/icons文件夹下\n2 将以下代码复制进assets/scss/custom.scss文件中(不存在则自行创建)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 .highlight { border-radius: var(--card-border-radius); max-width: 100% !important; margin: 0 !important; box-shadow: var(--shadow-l1) !important; } .highlight:before { content: \u0026#34;\u0026#34;; display: block; background: url(../icons/macOS-code-header.svg) no-repeat 0; background-size: contain; height: 18px; margin-top: -10px; margin-bottom: 10px; } 代码块展开\u0026amp;收起 (1) 准备一张向下展开图片(Ctrl+S保存)，放到assets/icons目录下\n(2) 将以下代码复制进layouts/partials/footer/custom.html(文件不存在则自行创建)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 \u0026lt;style\u0026gt; .highlight { /* 你可以根据需要调整这个高度 */ max-height: 400px; overflow: hidden; } .code-show { max-height: none !important; } .code-more-box { width: 100%; padding-top: 78px; background-image: -webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0)), to(#fff)); position: absolute; left: 0; right: 0; bottom: 0; z-index: 1; } .code-more-btn { display: block; margin: auto; width: 44px; height: 22px; background: #f0f0f5; border-top-left-radius: 8px; border-top-right-radius: 8px; padding-top: 6px; cursor: pointer; } .code-more-img { cursor: pointer !important; display: block; margin: auto; width: 22px; height: 16px; } \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; function initCodeMoreBox() { let codeBlocks = document.querySelectorAll(\u0026#34;.highlight\u0026#34;); if (!codeBlocks) { return; } codeBlocks.forEach(codeBlock =\u0026gt; { // 校验是否overflow if (codeBlock.scrollHeight \u0026lt;= codeBlock.clientHeight) { return; } // 元素初始化 // codeMoreBox let codeMoreBox = document.createElement(\u0026#39;div\u0026#39;); codeMoreBox.classList.add(\u0026#39;code-more-box\u0026#39;); // codeMoreBtn let codeMoreBtn = document.createElement(\u0026#39;span\u0026#39;); codeMoreBtn.classList.add(\u0026#39;code-more-btn\u0026#39;); codeMoreBtn.addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { codeBlock.classList.add(\u0026#39;code-show\u0026#39;); codeMoreBox.style.display = \u0026#39;none\u0026#39;; // 触发resize事件，重新计算目录位置 window.dispatchEvent(new Event(\u0026#39;resize\u0026#39;)) }) // img let img = document.createElement(\u0026#39;img\u0026#39;); img.classList.add(\u0026#39;code-more-img\u0026#39;); img.src = \u0026#34;{{ (resources.Get \u0026#34;icons/codeMore.png\u0026#34;).Permalink }}\u0026#34; // 元素添加 codeMoreBtn.appendChild(img); codeMoreBox.appendChild(codeMoreBtn); codeBlock.appendChild(codeMoreBox) }) } initCodeMoreBox(); \u0026lt;/script\u0026gt; 修改字体 (1) 前往100font下载自己想要的字体保存为ttf文件\n(2) 把字体文件放入assets/font下(文件夹自己创建)\n(3) 将以下代码修改并复制到layouts/partials/footer/custom.html文件中(文件不存在就自己创建)\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;style\u0026gt; @font-face { font-family: \u0026#39;字体名\u0026#39;; src: url({{ (resources.Get \u0026#34;font/字体文件名\u0026#34;).Permalink }}) format(\u0026#39;truetype\u0026#39;); } :root { --base-font-family: \u0026#39;字体名\u0026#39;; --code-font-family: \u0026#39;字体名\u0026#39;; } \u0026lt;/style\u0026gt; ","date":"2025-02-26T12:42:52+08:00","image":"https://Peraspera1.github.io/images/cybercover.jpg","permalink":"https://Peraspera1.github.io/p/%E8%B5%9B%E5%8D%9A%E8%A3%85%E4%BF%AE%E8%AE%B0%E5%BD%95/","title":"赛博装修记录"},{"content":"论文信息 信息概览 ECCV 2024 Oral Presentation\n论文题目： PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation\n论文单位： MIT\n是否开源： 是\n总结： 一种基于物理学的方法，通过利用视频生成模型学习的对象动力学先验，赋予静态3D对象交互式动力学，也就是使静态3D对象能够以物理上似乎合理的方式动态响应交互刺激。\n该方法使用3D高斯粒子表示物体，使用神经场建模材料属性，并通过可微分仿真（使用材料点法，MPM）模拟动态。\n论文思路 问题：给一张图片，比如一朵花，想知道这朵花在微风吹过后的动态信息，也就是求一个物体对于新物理交互的响应。\n但是求解这个响应，需要对物体的性质有较为准确的估计（比如两种弹性系数不同的弹簧，对其施加相同大小和方向的力，其变形显然是不一样的）。\n而这个性质是很难测量的，或者说难以形成大规模的数据集以供学习。\n但是人类能从观察物理世界和与物理世界互动中获得的物理先验知识，受此启发，作者从大量的视频先验中学习动力学先验，\n为了简化，这篇文章只对弹性物体做了仿真，那么估计的物理属性，有质量、杨氏模量和泊松比。质量等于密度乘体积，论文中粒子的体积是体素的体积除以其中包含的粒子数，密度是给定的常数，泊松比也是给定的常数，所以最后优化的只是一个杨氏模量的场。\n论文的关键思想是生成运动中物体的合理demo，比如一朵花，他把花离散为很多稠密的点，但这些点不是同构的，因此每个点的杨氏模量都不一样，然后按照物理属性去优化材质场E以匹配这个合成的运动。 我们首先从某个视点为 3D 场景出发渲染静态图像。然后，我们利用图像到视频模型(SVD)生成一个短视频剪辑 {$I_0$， $I_1$， . . . ， $I_T$ }，描绘对象的真实运动，这个生成的模型是GT来监督模拟得到的图像，然后再通过可微分模拟和可微渲染来优化材料场E(x)和初始速度场$v_0$(x)，使得模拟的渲染视频与生成的视频匹配。 但其实我觉得核心的部分还是图中下面的箭头，也就是PhysGaussian的工作比较重要。\n细节部分 仿照PhysGaussian内部填充？\n$$ \\rho \\frac{D v}{D t} = \\nabla \\cdot \\sigma + f, \\frac{D \\rho}{D t} + \\rho \\nabla \\cdot v = 0 $$v 是欧拉视角，密度是常量，f是外力。\nMPM的实现细节需要单独花时间细看。\n$$ x^{t+1}, v^{t+1}, F^{t+1}, C^{t+1} = S(x^{t}, v^{t}, F^{t}, C^{t}, \\theta , \\Delta t) $$F和C分别是局部变形场的梯度和应力场的梯度，$\\theta$ 代表所有的物理量，在文章里代表E，$\\Delta \\approx 1 \\times 10^{-4}$，仿真了100步。\n$$ \\hat{I}^t = F_{render}(x^t, \\alpha, R^t, \\Sigma, c) $$R代表所有粒子的旋转矩阵，\n$$ L^t = \\lambda L_1(\\hat{I}^t, I^t) + (1-\\lambda)L_{D-SSIM}(\\hat{I}^t, I^t) $$创新点(MPM加速) 高斯模型包含成千上万个点，这对于模拟来说效率较低。因此，本文采用了下采样方法，每个下采样后的点能够有效描述其对应领域的信息。此外，下采样对3D几何形状（3DGS）的表征同样至关重要。因为3DGS表征存在过于局部化的问题（不同区域之间的表征可能会出现突变或不连贯），这会导致空间表征的不连续性。通过下采样后，每个点包含了其领域的信息，从而有可能推动表征向混合高斯模型（mixture-Gaussian）方向发展，使得空间的整体表示更加连续。这样的方法可能为将三维场景表示为一串序列提供了思路，可以进一步应用于MLLM。例如，可以将该序列视作一个Encoder-Decoder模型，并通过重建信息作为监督信号进行训练。\n结果 数据集：八个真实场景，大部分是花，这个作为对照组\n其实没什么意义，因为本身这篇论文是在前两篇的基础上做的，而且PhysGaussian没有优化物理参数，DreamGaussian没有物理假设。。\n讨论 视频生成的方式 用SVD采样得到了14帧的信息作为监督。\n疑问的点，text-prompt怎么设计，比如花在空中摇摆，或者被人碰了一下，怎么去量化这个幅度？\nloss的设计 用生成得到的视频监督是否合理？ 因为整个3DGS的参数很多，这篇文章只是监督了E，而其他的位置，速度等信息都是仿真算出来的，所以DoF，或者说优化的参数空间其实比较小。但是如果要学习更多的物理信息，只用SVD去监督肯定不合理。\n其次，生成得到的视频离真实场景还是有差别，所以还是要做一个trade-off，一种方法是减少生成视频对模型的影响，例如使用结构损失作为损失函数，或者将生成的视频帧作为guidance来进行distillation，另一种是降低估计的Dof，想这篇文章做的那样，固定泊松比和质量，只估计杨氏模量，第三种方式是提高视频生成的能力，脱离SDS损失函数的监督，转向全监督学习，即让生成的视频与真实场景之间有更多直接的监督。\n","date":"2025-02-25T00:00:00Z","image":"https://Peraspera1.github.io/images/physdreamer_cover.jpg","permalink":"https://Peraspera1.github.io/p/physdreamer/","title":"Physdreamer"},{"content":"论文信息 信息概览 CVPR 2024 Highlight\n论文题目： PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics\n论文单位： UCLA\n是否开源： 是\n总结： PhysGaussian是第一篇将3D高斯核（用于渲染）与物理属性（如速度、应变、应力）相结合，使其能够模拟牛顿力学中的动态行为，适用于多种不同材料（如弹性物体、金属、非牛顿流体等）。\n论文思路 核心思想是what you see is what you simulate。\n传统的图形学物理引擎往往会导致模拟与可视化之间的差别，但自然界中材料的物理特性与视觉外观本质上也是交织在一起的，受此启发，本文将物理学赋予 3D 高斯核，赋予它们运动学属性（如速度和应变）以及机械属性（如弹性能、应力和塑性），进而弥补这一差别。\n代码debug记录 ","date":"2025-02-19T00:00:00Z","image":"https://Peraspera1.github.io/images/physgaussian_cover.jpg","permalink":"https://Peraspera1.github.io/p/physgaussian/","title":"Phygaussian"},{"content":"论文信息 CVPR 2024 Best Paper\n论文题目： Generative Image Dynamics\n是否开源： 否\n总结：\n一种从静态图像生成动态运动的方法，特别是针对自然的振荡运动，如树木在风中摇动、花朵或蜡烛火焰的摆动等。 核心思想是使用频谱体积，这是一种基于频率的像素运动表示，通过真实视频序列进行学习。\n细节 输入单张图片$I_0$, 输出${\\hat{I}_1, \\hat{I}_2, ..., \\hat{I}_T}$，\n用LDM从输入的单张图片中预测谱体积 $\\mathcal{S}=\\left(S_{f_0},S_{f_1},...,S_{f_{K-1}}\\right)$ ，然后再利用这个谱体积恢复$\\mathcal{F}=(F_1,F_2,...,F_T)$，即后面T个时刻的运动。$F_t(p)$ 代表t时刻$I_0$中第p个像素的位置。（$I_t^\\prime(\\mathbf{p}+F_t(\\mathbf{p}))=I_0(\\mathbf{p})$）\nspectral volume（谱体积）： 从视频中提取的每像素轨迹的时间傅里叶变换。\n频谱分析-\u0026gt;解决生成视频的长期时间一致性??\nk个频率，(x,y)+- 共4K个channel\n对于一张 $H \\times W$的图片，每个像素p可以表示为$I_{t}(p) = \\Sigma_{k} [Asin(k \\omega x t) + B cos(k \\omega x t)]$\n值得阅读的参考论文\nMyers Abraham Davis. Visual vibration analysis. PhD thesis, Massachusetts Institute of Technology, 2016.\n","date":"2025-02-17T00:00:00Z","image":"https://Peraspera1.github.io/post/GID/GIDfront.jpg","permalink":"https://Peraspera1.github.io/p/generativeid/","title":"Generative Image Dynamics"},{"content":"跨实体灵巧抓取的机器人与物体交互的统一表示 ","date":"2024-12-19T00:00:00Z","image":"https://Peraspera1.github.io/p/dro/DROfront.png","permalink":"https://Peraspera1.github.io/p/dro/","title":"D(R,O) Grasp"},{"content":" hello hugo\n","date":"2024-12-18T00:00:00Z","permalink":"https://Peraspera1.github.io/p/huber/","title":"Huber"},{"content":"Mast3r - SLAM 定义 dust3r/mast3r 输入$\\mathcal{I}^i,\\mathcal{I}^j\\in\\mathbb{R}^{H\\times W\\times3}$\n得到$\\mathbf{X}_i^i,\\mathbf{X}_i^j\\in\\mathbb{R}^{H\\times W\\times3}$及对应的置信度 $\\mathrm{C}_i^i,\\mathrm{C}_i^j\\in\\mathbb{R}^{H\\times W\\times1}$\n$\\mathbf{X}_j^i$是图片i的点云在相机j下面的坐标\n相比于dust3r,mast3r还输出了每个像素的特征向量$\\mathrm{D}_i^i,\\mathrm{D}_i^j\\in\\mathbb{R}^{H\\times W\\times d}$，及$\\mathrm{Q}_i^i,\\mathrm{Q}_i^j\\in\\mathbb{R}^{H\\times W\\times1}$。类似于特征子和描述子。\n将mast3r的输出结果合并记为：$\\mathcal{F}_M(\\mathcal{I}^i,\\mathcal{I}^j)$\n一对图片的匹配集合表示为：$\\mathbf{m}_{i,j}=\\mathcal{M}(\\mathbf{X}_i^i,\\mathbf{X}_i^j,\\mathbf{D}_i^i,\\mathbf{D}_i^j)$\n整体思路 1 前端优化：逐帧优化相机位姿，构建局部地图。\n2 后端优化：全局优化所有关键帧和地图点，修正漂移和保证全局一致性。\n前端优化 目标有两个：\n1 在新帧和最近关键帧之间进行相机位姿的局部优化。\n2 在局部范围内融合点地图（点云融合）。\n点图匹配 为了进行局部的位姿优化，首先需要找到两张图之间的对应点：\nprojective data-association：\n利用相机投影模型（例如针孔模型或其他中心投影模型，本文用的是射线），将 3D 点投影到图像平面上，并找到对应的像素位置。在投影到的像素周围局部搜索对应的观测点（例如通过颜色、特征等进行匹配）。 通过这种投影与局部搜索，实现高效且准确的点匹配，而不需要全局特征匹配（如耗时的 brute-force 特征匹配）。\n但是这个模型需要准确的相机参数-\u0026gt;构建相机模型\ndef : 对于一幅图像中的点云，光线定义为从相机中心指向某个 3D 点的单位向量。 作者采用的方式：基于迭代优化求解，最小化投影光线之间的角度误差优化的参数是： $\\mathbf{T}= \\begin{bmatrix} s\\mathbf{R} \u0026 \\mathbf{t} \\\\ 0 \u0026 1 \\end{bmatrix}$， 及图片之间的像素的匹配关系\n$$\\mathbf{p}^*=\\arg\\min_\\mathbf{p}\\left\\|\\psi\\left([\\mathbf{X}_i^i]_\\mathbf{p}\\right)-\\psi\\left(\\mathbf{x}\\right)\\right\\|^2.$$$$\\left\\|\\psi_1-\\psi_2\\right\\|^2=2(1-\\cos\\theta),\\quad\\cos\\theta=\\psi_1^T\\psi_2.$$\n只有外参和尺度，没有估计内参，利用cuda并行加速\n过滤点的方式：过滤三维空间中距离相差较大的点(文中没有提到具体的做法)\n点图匹配的优化方法 目标： 找到两张图片之间的对应点\n初始化：恒等映射，即按像素坐标映射\n优化的方程：\n$$\\mathbf{p}^*=\\arg\\min_\\mathbf{p}\\left\\|\\psi\\left([\\mathbf{X}_i^i]_\\mathbf{p}\\right)-\\psi\\left(\\mathbf{x}\\right)\\right\\|^2.$$优化的变量：以第一张图片为参考系，逐像素优化第二张图片的对应点的像素坐标(u,v)，这个过程可以在GPU上并行运算(cuda)，并且文中提到只要10次迭代就可以收敛(速度比mast3r找对应点的方式更快吗?)\n跟踪 估计当前帧(f)和最后一个关键帧(k)的相对位姿变换\n什么叫使用网络的单次传递来估计变换？\n损失函数: mast3r会估计出图片1的在图片2坐标系下的点云、图片2对应的点云在图片2坐标系下的点云（反过来也是一样的），这两个点云只相差了一个坐标变换矩阵（以及尺度？同一个pair下面也会有尺度不一致吗？）。这个损失函数应该只考虑了置信度较高的特征点对应的像素对应的点云。\n$$E_p=\\sum_{m,n\\in\\mathbf{m}_{f,k}}\\left\\|\\frac{\\tilde{\\mathbf{X}}_{k,n}^k-\\mathrm{T}_{kf}\\mathrm{X}_{f,m}^f}{w(\\mathrm{q}_{m,n},\\sigma_p^2)}\\right\\|_\\rho$$$$\\mathrm{q}_{m,n}=\\sqrt{\\mathrm{Q}_{f,m}^f\\mathrm{Q}_{f,n}^k}$$ q是描述子的内积; (这个$\\sigma$没有定义？)\n$$w(\\mathbf{q},\\sigma^2)= \\begin{cases} \\sigma^2/\\mathbf{q} \u0026 \\mathbf{q}\u003e\\mathbf{q}_{min} \\\\ \\infty \u0026 \\mathrm{otherwise} \u0026 \u0026 \\end{cases}.$$如何解决3D点深度不一致的问题-\u0026gt;光线误差（而不是重投影）+点云融合\n利用射线的角度误差而非投影误差（这个误差对深度不敏感）\n$$E_r=\\sum_{m,n\\in\\mathbf{m}_{f,k}}\\left\\|\\frac{\\psi\\left(\\tilde{\\mathbf{X}}_{k,n}^k\\right)-\\psi\\left(\\mathbf{T}_{kf}\\mathbf{X}_{f,m}^f\\right)}{w(\\mathbf{q}_{m,n},\\sigma_r^2)}\\right\\|_\\rho.$$角度误差是有界的，基于射线的误差对异常值具有鲁棒性。因为：1.相比直接使用 3D 点误差，射线误差（角度误差）更鲁棒，因为它只考虑方向，而不依赖于深度的绝对精度。 避免尺度问题：2.由于单目 SLAM 中的尺度不确定性，使用射线误差能够更好地处理尺度问题。\n我们还包括一个关于距离相机中心的距离差的小权重的误差项。这防止了系统在纯旋转情况下退化，但较小的权重避免了像点位误差那样对位姿估计产生偏差。？？？\n点云融合 $$\\tilde{\\mathbf{X}}_{k}^{k}\\leftarrow\\frac{\\tilde{\\mathbb{C}}_{k}^{k}\\tilde{\\mathbb{X}}_{k}^{k}+\\mathbb{C}_{f}^{k}\\left(\\mathbb{T}_{kf}\\mathbb{X}_{f}^{k}\\right)}{\\tilde{\\mathbb{C}}_{k}^{k}+\\mathbb{C}_{f}^{k}},\\tilde{\\mathbb{C}}_{k}^{k}\\leftarrow\\tilde{\\mathbb{C}}_{k}^{k}+\\mathbb{C}_{f}^{k}.$$置信度直接相加吗？-\u0026gt;某一个点的置信度会越来越高\n后端优化 图的构建以及回环检测 怎么判断一个图片是否是关键帧？\n通过mast3r构建一个pair，同时输入最后一个关键帧和当前的图片，计算匹配的像素点的数量，如果这个数量低于某一个值，那就说明这个新加入的图片能够产生足够多的新的点云，因此就把这张图作为关键帧。\ngraph的顺序是什么？\n按照时间顺序串联，同时维护一个边的集合，每次加入两个关键帧之间的边（双向）\n局部的回环检测和全局的回环检测\n采用MASt3R-SfM使用的聚合选择性匹配内核 (ASMK)框架(?)，用于从编码特征进行图像检索。 这样就能找到两张类似的图片来进行回环检测。（但是文章中没有提到增量式建立ASMK框架的过程以及检测到回环之后调整全局位姿的方法？）\n后端优化 后端优化的方法，对于边的集合中所有的图片一起进行优化\n$$E_g=\\sum_{i,j\\in\\mathcal{E}}\\sum_{m,n\\in\\mathbf{m}_{i,j}}\\left\\|\\frac{\\psi\\left(\\tilde{\\mathbf{X}}_{i,m}^i\\right)-\\psi\\left(\\mathbf{T}_{ij}\\tilde{\\mathbf{X}}_{j,n}^j\\right)}{w(\\mathbf{q}_{m,n},\\sigma_r^2)}\\right\\|_\\rho$$假设集合中有N个关键帧，那么形成了2N个边（双向边，形成了回环），每个关键帧有7个自由度（3 个旋转、3 个平移和 1 个尺度）\n数学公式推导 ","date":"2024-12-17T00:00:00Z","image":"https://Peraspera1.github.io/p/mast3rslam/image.png","permalink":"https://Peraspera1.github.io/p/mast3rslam/","title":"Mast3r-slam"}]